{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Stopword removal **"
      ],
      "metadata": {
        "id": "mM9dVCOnElGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jIsS4sxbEBT4"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "7l3r6vZEEB9b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuCmDCY3EHjc",
        "outputId": "f3a6d362-344b-4c15-b717-c75a2ba10e6f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words(\"english\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIsKmqpcEJok",
        "outputId": "4a9bbb22-ec20-45d6-9a7d-ffbbf52c0fe7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    new_text=[]\n",
        "\n",
        "    for word in text.split():\n",
        "        if word in stopwords.words(\"english\"):\n",
        "            new_text.append(\"\")\n",
        "        else:\n",
        "            new_text.append(word.strip())\n",
        "\n",
        "\n",
        "    return \" \".join(new_text).replace(\"   \",\"\")"
      ],
      "metadata": {
        "id": "bGh5GRVnEL1V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"i am darshan and we are learning nlp and here is why nlp are important\""
      ],
      "metadata": {
        "id": "AypxsdMAEQ5x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_stopwords(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R_jmzHedEU1Z",
        "outputId": "2ea52dee-8752-4400-8605-93a199c2dd9e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  darshan learning nlp  nlp  important'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "\n",
        "  new_text=[]\n",
        "  for word in text.split():\n",
        "    if word not in Stopword:\n",
        "      new_text.append(word)\n",
        "  return \" \".join(new_text)\n"
      ],
      "metadata": {
        "id": "jkTUNv1NEW9h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vMv7FZghEZFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization **"
      ],
      "metadata": {
        "id": "7om3Y0fVSX-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"i am darshan and i am working as a data scientist\""
      ],
      "metadata": {
        "id": "fFUA1YFsSbMl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FND0pNXVSo8u",
        "outputId": "9429ad52-c09c-4e77-d1bb-a1bcfff700e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'am',\n",
              " 'darshan',\n",
              " 'and',\n",
              " 'i',\n",
              " 'am',\n",
              " 'working',\n",
              " 'as',\n",
              " 'a',\n",
              " 'data',\n",
              " 'scientist']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"i am dd and working as a data scientist. i live in pune. i work in mathco.\""
      ],
      "metadata": {
        "id": "NmJf4GgUSuvg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU7HBsWPS3C_",
        "outputId": "e9b49351-b057-4e16-e463-cdff06655159"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i am dd and working as a data scientist',\n",
              " ' i live in pune',\n",
              " ' i work in mathco',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4=\"where should i go? i habve 3 days hoilidays, Can you sugguest me?\""
      ],
      "metadata": {
        "id": "dKZbd8WsS4lr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text4.split(\".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtQXkY4ETNPC",
        "outputId": "6dba5492-8e05-48db-d38f-bcc229ead648"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['where should i go? i habve 3 days hoilidays, Can you sugguest me?']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text3=\"i live in pune!!\""
      ],
      "metadata": {
        "id": "yiwaoor2TPfY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[\\w]+\",text3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S31v8Zm-TVnZ",
        "outputId": "83c5a58f-ad0f-48cf-8426-af70c5dbb315"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'live', 'in', 'pune']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizing using NLTK**"
      ],
      "metadata": {
        "id": "FsZBnArcWAiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_text=\"i am going to visit delhi tomorrow evening\""
      ],
      "metadata": {
        "id": "_-vDzJd0TXYH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOgkKJ5UWM70",
        "outputId": "c4802850-8f62-49f3-baf3-062f18f0cd06"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "word_tokenize(my_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnpfB-cJWP2O",
        "outputId": "b70f6e34-fbde-4376-f93b-af0eef925f6f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'going', 'to', 'visit', 'delhi', 'tomorrow', 'evening']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_corpus=\"\"\"Generative artificial intelligence (generative AI, genAI, GenAI, GAI or GenAI[1]) is artificial intelligence capable of generating text, images or other data using generative models,[2] often in response to prompts.[3][4] Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.[5][6]\n",
        "\n",
        "Improvements in transformer-based deep neural networks enabled an AI boom of generative AI systems in the early 2020s. These include large language model (LLM) chatbots such as ChatGPT, Copilot, Bard, and LLaMA, and text-to-image artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL-E.[7][8][9] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[3][10][11]\"\"\""
      ],
      "metadata": {
        "id": "d5dBVYauWTRA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "ArXo7NtcWwNw",
        "outputId": "2459b65d-b6b4-4400-d2dc-f0651cbf30a3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Generative artificial intelligence (generative AI, genAI, GenAI, GAI or GenAI[1]) is artificial intelligence capable of generating text, images or other data using generative models,[2] often in response to prompts.[3][4] Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.[5][6]\\n\\nImprovements in transformer-based deep neural networks enabled an AI boom of generative AI systems in the early 2020s. These include large language model (LLM) chatbots such as ChatGPT, Copilot, Bard, and LLaMA, and text-to-image artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL-E.[7][8][9] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[3][10][11]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(my_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmDFuDZ_WyUS",
        "outputId": "a8dd2581-6ec7-45db-a807-749ac55eae9d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Generative artificial intelligence (generative AI, genAI, GenAI, GAI or GenAI[1]) is artificial intelligence capable of generating text, images or other data using generative models,[2] often in response to prompts.',\n",
              " '[3][4] Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.',\n",
              " '[5][6]\\n\\nImprovements in transformer-based deep neural networks enabled an AI boom of generative AI systems in the early 2020s.',\n",
              " 'These include large language model (LLM) chatbots such as ChatGPT, Copilot, Bard, and LLaMA, and text-to-image artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL-E.[7][8][9] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.',\n",
              " '[3][10][11]']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PCvUuOWW0sa",
        "outputId": "9ba5c676-bebb-4a5a-dd4d-ab344f547dbd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English NLP model from spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "St89JdHzZSAp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    # Process the text with the spacy model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract tokens from the processed document\n",
        "    tokens = [token.text for token in doc]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "6p06gfdWZmvk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=tokenize_text(my_corpus)"
      ],
      "metadata": {
        "id": "vd5kKrHLZqUr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxFSREihZszl",
        "outputId": "7a021e61-028b-400e-d3c2-e35992f5386a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Generative',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " '(',\n",
              " 'generative',\n",
              " 'AI',\n",
              " ',',\n",
              " 'genAI',\n",
              " ',',\n",
              " 'GenAI',\n",
              " ',',\n",
              " 'GAI',\n",
              " 'or',\n",
              " 'GenAI[1',\n",
              " ']',\n",
              " ')',\n",
              " 'is',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'capable',\n",
              " 'of',\n",
              " 'generating',\n",
              " 'text',\n",
              " ',',\n",
              " 'images',\n",
              " 'or',\n",
              " 'other',\n",
              " 'data',\n",
              " 'using',\n",
              " 'generative',\n",
              " 'models,[2',\n",
              " ']',\n",
              " 'often',\n",
              " 'in',\n",
              " 'response',\n",
              " 'to',\n",
              " 'prompts.[3][4',\n",
              " ']',\n",
              " 'Generative',\n",
              " 'AI',\n",
              " 'models',\n",
              " 'learn',\n",
              " 'the',\n",
              " 'patterns',\n",
              " 'and',\n",
              " 'structure',\n",
              " 'of',\n",
              " 'their',\n",
              " 'input',\n",
              " 'training',\n",
              " 'data',\n",
              " 'and',\n",
              " 'then',\n",
              " 'generate',\n",
              " 'new',\n",
              " 'data',\n",
              " 'that',\n",
              " 'has',\n",
              " 'similar',\n",
              " 'characteristics.[5][6',\n",
              " ']',\n",
              " '\\n\\n',\n",
              " 'Improvements',\n",
              " 'in',\n",
              " 'transformer',\n",
              " '-',\n",
              " 'based',\n",
              " 'deep',\n",
              " 'neural',\n",
              " 'networks',\n",
              " 'enabled',\n",
              " 'an',\n",
              " 'AI',\n",
              " 'boom',\n",
              " 'of',\n",
              " 'generative',\n",
              " 'AI',\n",
              " 'systems',\n",
              " 'in',\n",
              " 'the',\n",
              " 'early',\n",
              " '2020s',\n",
              " '.',\n",
              " 'These',\n",
              " 'include',\n",
              " 'large',\n",
              " 'language',\n",
              " 'model',\n",
              " '(',\n",
              " 'LLM',\n",
              " ')',\n",
              " 'chatbots',\n",
              " 'such',\n",
              " 'as',\n",
              " 'ChatGPT',\n",
              " ',',\n",
              " 'Copilot',\n",
              " ',',\n",
              " 'Bard',\n",
              " ',',\n",
              " 'and',\n",
              " 'LLaMA',\n",
              " ',',\n",
              " 'and',\n",
              " 'text',\n",
              " '-',\n",
              " 'to',\n",
              " '-',\n",
              " 'image',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'art',\n",
              " 'systems',\n",
              " 'such',\n",
              " 'as',\n",
              " 'Stable',\n",
              " 'Diffusion',\n",
              " ',',\n",
              " 'Midjourney',\n",
              " ',',\n",
              " 'and',\n",
              " 'DALL',\n",
              " '-',\n",
              " 'E.[7][8][9',\n",
              " ']',\n",
              " 'Companies',\n",
              " 'such',\n",
              " 'as',\n",
              " 'OpenAI',\n",
              " ',',\n",
              " 'Anthropic',\n",
              " ',',\n",
              " 'Microsoft',\n",
              " ',',\n",
              " 'Google',\n",
              " ',',\n",
              " 'and',\n",
              " 'Baidu',\n",
              " 'as',\n",
              " 'well',\n",
              " 'as',\n",
              " 'numerous',\n",
              " 'smaller',\n",
              " 'firms',\n",
              " 'have',\n",
              " 'developed',\n",
              " 'generative',\n",
              " 'AI',\n",
              " 'models.[3][10][11',\n",
              " ']']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming and lemmatization **"
      ],
      "metadata": {
        "id": "cFbvFRfQaqe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stemming(text):\n",
        "    obj=PorterStemmer()\n",
        "\n",
        "    stem_word=[obj.stem(word) for word in text.split()]\n",
        "\n",
        "    return stem_word"
      ],
      "metadata": {
        "id": "bYJPeo9yZvf_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"The quick brown foxes are jumping over the lazy dogs\""
      ],
      "metadata": {
        "id": "PuVidZ0Pa7FL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4AeZqxLza9q9",
        "outputId": "2b11bc55-2681-46ac-e210-e30701a40522"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i am dd and working as a data scientist. i live in pune. i work in mathco.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSmQgk5qbBUd",
        "outputId": "8b4b6a3c-2471-44ea-877b-7aaab48e1cd0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'am',\n",
              " 'dd',\n",
              " 'and',\n",
              " 'work',\n",
              " 'as',\n",
              " 'a',\n",
              " 'data',\n",
              " 'scientist.',\n",
              " 'i',\n",
              " 'live',\n",
              " 'in',\n",
              " 'pune.',\n",
              " 'i',\n",
              " 'work',\n",
              " 'in',\n",
              " 'mathco.']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIeirLajbEsB",
        "outputId": "8aca8e48-6e03-4323-c749-42700d97da77"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lammatization(text):\n",
        "    words=text.split()\n",
        "\n",
        "    lemmetizer=WordNetLemmatizer()\n",
        "\n",
        "    lemetized_word=[lemmetizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return lemetized_word"
      ],
      "metadata": {
        "id": "g9-tm6UXbITh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lammatization(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjdZfIpSbLIM",
        "outputId": "b3856925-d069-4b4f-a6e3-371bf499e2ff"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'am',\n",
              " 'dd',\n",
              " 'and',\n",
              " 'working',\n",
              " 'a',\n",
              " 'a',\n",
              " 'data',\n",
              " 'scientist.',\n",
              " 'i',\n",
              " 'live',\n",
              " 'in',\n",
              " 'pune.',\n",
              " 'i',\n",
              " 'work',\n",
              " 'in',\n",
              " 'mathco.']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhMkfOCFbP7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}